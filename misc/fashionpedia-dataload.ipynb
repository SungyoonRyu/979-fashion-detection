{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = \"./data/fashionpedia/\"\n",
    "train_path = \"train2020\"\n",
    "test_path = \"test2020\"\n",
    "anno_file_path = \"annotations\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root_dir, set='train2017', transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.set_name = set\n",
    "        self.transform = transform\n",
    "\n",
    "        # TODO: change instatnce string\n",
    "        self.coco = COCO(os.path.join(self.root_dir, 'annotations', 'instances_attributes_' + self.set_name + '.json'))\n",
    "        self.image_ids = self.coco.getImgIds()\n",
    "\n",
    "        self.load_classes()\n",
    "\n",
    "    def load_classes(self):\n",
    "\n",
    "        # load class names (name -> label)\n",
    "        categories = self.coco.loadCats(self.coco.getCatIds())\n",
    "        categories.sort(key=lambda x: x['id'])\n",
    "\n",
    "        self.classes = {}\n",
    "        for c in categories:\n",
    "            self.classes[c['name']] = len(self.classes)\n",
    "\n",
    "        # also load the reverse (label -> name)\n",
    "        self.labels = {}\n",
    "        for key, value in self.classes.items():\n",
    "            self.labels[value] = key\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = self.load_image(idx)\n",
    "        annot = self.load_annotations(idx)\n",
    "        sample = {'img': img, 'annot': annot}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n",
    "        path = os.path.join(self.root_dir, self.set_name, image_info['file_name'])\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return img.astype(np.float32) / 255.\n",
    "\n",
    "    def load_annotations(self, image_index):\n",
    "        # get ground truth annotations\n",
    "        annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)\n",
    "        annotations = np.zeros((0, 5))\n",
    "\n",
    "        # some images appear to miss annotations\n",
    "        if len(annotations_ids) == 0:\n",
    "            return annotations\n",
    "\n",
    "        # parse annotations\n",
    "        coco_annotations = self.coco.loadAnns(annotations_ids)\n",
    "        for idx, a in enumerate(coco_annotations):\n",
    "\n",
    "            # some annotations have basically no width / height, skip them\n",
    "            if a['bbox'][2] < 1 or a['bbox'][3] < 1:\n",
    "                continue\n",
    "\n",
    "            annotation = np.zeros((1, 5))\n",
    "            annotation[0, :4] = a['bbox']\n",
    "            annotation[0, 4] = a['category_id'] - 1\n",
    "            annotations = np.append(annotations, annotation, axis=0)\n",
    "\n",
    "        # transform from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]\n",
    "        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]\n",
    "\n",
    "        return annotations\n",
    "\n",
    "def collater(data):\n",
    "    imgs = [s['img'] for s in data]\n",
    "    annots = [s['annot'] for s in data]\n",
    "    scales = [s['scale'] for s in data]\n",
    "\n",
    "    imgs = torch.from_numpy(np.stack(imgs, axis=0))\n",
    "\n",
    "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "\n",
    "        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "        for idx, annot in enumerate(annots):\n",
    "            if annot.shape[0] > 0:\n",
    "                annot_padded[idx, :annot.shape[0], :] = annot\n",
    "    else:\n",
    "        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "    imgs = imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {'img': imgs, 'annot': annot_padded, 'scale': scales}\n",
    "\n",
    "\n",
    "class Resizer(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=512):\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "        height, width, _ = image.shape\n",
    "        if height > width:\n",
    "            scale = self.img_size / height\n",
    "            resized_height = self.img_size\n",
    "            resized_width = int(width * scale)\n",
    "        else:\n",
    "            scale = self.img_size / width\n",
    "            resized_height = int(height * scale)\n",
    "            resized_width = self.img_size\n",
    "\n",
    "        image = cv2.resize(image, (resized_width, resized_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        new_image = np.zeros((self.img_size, self.img_size, 3))\n",
    "        new_image[0:resized_height, 0:resized_width] = image\n",
    "\n",
    "        annots[:, :4] *= scale\n",
    "\n",
    "        return {'img': torch.from_numpy(new_image).to(torch.float32), 'annot': torch.from_numpy(annots), 'scale': scale}\n",
    "\n",
    "\n",
    "class Augmenter(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, flip_x=0.5):\n",
    "        if np.random.rand() < flip_x:\n",
    "            image, annots = sample['img'], sample['annot']\n",
    "            image = image[:, ::-1, :]\n",
    "\n",
    "            rows, cols, channels = image.shape\n",
    "\n",
    "            x1 = annots[:, 0].copy()\n",
    "            x2 = annots[:, 2].copy()\n",
    "\n",
    "            x_tmp = x1.copy()\n",
    "\n",
    "            annots[:, 0] = cols - x2\n",
    "            annots[:, 2] = cols - x_tmp\n",
    "\n",
    "            sample = {'img': image, 'annot': annots}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "\n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.mean = np.array([[mean]])\n",
    "        self.std = np.array([[std]])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "\n",
    "        return {'img': ((image.astype(np.float32) - self.mean) / self.std), 'annot': annots}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=7.43s)\n",
      "creating index...\n",
      "index created!\n",
      "len(dataset)=45623\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms\n",
    "from torchvision.transforms import *\n",
    "\n",
    "# training_set = CocoDataset(root_dir=os.path.join(opt.data_path, params.project_name), set=params.train_set,\n",
    "#                             transform=transforms.Compose([Normalizer(mean=params.mean, std=params.std),\n",
    "#                                                             Augmenter(),\n",
    "#                                                             Resizer(input_sizes[opt.compound_coef])]))\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "compound_coef=0\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "dataset = CocoDataset(root_dir=data_path, set=train_path, transform=transforms.Compose([Normalizer(mean=mean, std=std),\n",
    "                                                            Augmenter(),\n",
    "                                                            Resizer(input_sizes[compound_coef])]))\n",
    "print(f'{len(dataset)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 682, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.load_image(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=6,shuffle=True, collate_fn=collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "I = next(iter(dataloader))\n",
    "print(I['scale'])\n",
    "\n",
    "# resize 해야 batch가 묶임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class FashionpediaDataset(Dataset):\n",
    "    def __init__(self, root_path, set_name, transform):\n",
    "        self.root_path = root_path\n",
    "        self.set_name = set_name\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.coco = COCO(os.path.join(root_path, 'annotations', f'instances_attributes_{set_name}.json'))\n",
    "        self.image_ids = self.coco.getImgIds()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(idx)\n",
    "        annotations = self.load_annotations(idx)\n",
    "        \n",
    "        if self.transform:\n",
    "            img, annotations = self.transform(img, annotations)\n",
    "        \n",
    "        return torch.from_numpy(img), torch.from_numpy(annotations)\n",
    "    \n",
    "    def load_image(self, idx) -> np.array:\n",
    "        image_info = self.coco.loadImgs(self.image_ids[idx])[0]\n",
    "        path = os.path.join(self.root_path, self.set_name, image_info['file_name'])\n",
    "        image = cv2.imread(path)\n",
    "        \n",
    "        return image.astype(np.float32) / 255.\n",
    "    \n",
    "    def load_annotations(self, idx) -> np.array:\n",
    "        annotation_ids = self.coco.getAnnIds(self.image_ids[idx])\n",
    "        annotations = np.zeros((0, 5))\n",
    "        \n",
    "        if len(annotation_ids) == 0:\n",
    "            return annotations\n",
    "        \n",
    "        coco_annotations = self.coco.loadAnns(self.image_ids[idx])\n",
    "        \n",
    "        for anno in coco_annotations:\n",
    "            \n",
    "            if anno['bbox'][2] < 1 or anno['bbox'][3] < 1:\n",
    "                continue\n",
    "            \n",
    "            annotation = np.zeros((1, 5))\n",
    "            annotation[0, :4] = anno['bbox']\n",
    "            annotation[0, 4] = anno['category_id'] - 1\n",
    "            annotations = np.append(annotations, annotation, axis=0)\n",
    "        \n",
    "        return annotations\n",
    "    \n",
    "def collate_fn(batch):\n",
    "        im, label = zip(*batch)  # transposed\n",
    "        for i, lb in enumerate(label):\n",
    "            lb[:, 0] = i  # add target image index for build_targets()\n",
    "        return torch.stack(im, 0), torch.cat(label, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.56s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = FashionpediaDataset(root_path=data_path, set_name=train_path, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1024, 1024, 3] at entry 0 and [1024, 682, 3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\myLaptop\\fashion\\979-fashion-detection\\misc\\fashionpedia-dataload.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/myLaptop/fashion/979-fashion-detection/misc/fashionpedia-dataload.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32mc:\\myLaptop\\fashion\\979-fashion-detection\\misc\\fashionpedia-dataload.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myLaptop/fashion/979-fashion-detection/misc/fashionpedia-dataload.ipynb#X20sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, lb \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(label):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/myLaptop/fashion/979-fashion-detection/misc/fashionpedia-dataload.ipynb#X20sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     lb[:, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m i  \u001b[39m# add target image index for build_targets()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/myLaptop/fashion/979-fashion-detection/misc/fashionpedia-dataload.ipynb#X20sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(im, \u001b[39m0\u001b[39;49m), torch\u001b[39m.\u001b[39mcat(label, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1024, 1024, 3] at entry 0 and [1024, 682, 3] at entry 1"
     ]
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
